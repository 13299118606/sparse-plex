
 
Generalized PCA
----------------------------------------------------


.. contents::
    :local:



Generalized Principal Component Analysis (GPCA)
is algebraic subspace clustering technique based on 
polynomial fitting and differentiation 
:cite:`vidal2003generalized,vidal2004motion,huang2004minimum,vidal2005generalized,vidal2008multiframe`.
The basic idea is that a union of subspaces can be
represented as a zero set of a set of homogeneous polynomials.
Once the set of polynomials has been fitted for the given dataset,
individual component subspaces can be identified via polynomial
differentiation and division. See :ref:`here <sec:algebraic_geometry>`
for a quick review of ideas from algebraic geometry which are
used in the development of GPCA algorithm.

We will assume that :math:`\UUU_k` are linear subspaces. If they
are affine, we simply take their homogeneous embeddings.

 
Representing the union of subspaces with a set of homogeneous polynomials
""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""

Consider the :math:`k`-th subspace :math:`\UUU_k \subset \RR^M` with dimension
:math:`D_k` and its orthogonal complement :math:`\UUU_k^{\perp}` with
dimension :math:`D'_k = M - D_k`.
Choose a basis for :math:`\UUU_k^{\perp}` as:


.. math::
    B_k = [b_{k_1}, \dots, b_{k_{D'_k}}] \in \RR^{M \times D'_k}.

Recall that for each :math:`y \in \UUU_k`, :math:`b_{k_i}^T y = 0` as 
vectors in :math:`\UUU_k^{\perp}` are orthogonal to vectors in :math:`\UUU_k`.
Note that each of the forms :math:`b_{k_i}^T y` is a homogeneous polynomial
of degree 1. The solutions of :math:`b_{k_i}^T y = 0` are (linear) hyperplanes of 
dimension :math:`M-1` and the subspace :math:`\UUU_k` 
is the intersection of these hyperplanes. In other words:


.. math::
    \begin{aligned}
    \UUU_k &= \{ y \in \RR^M : B_k^T y  = 0 \}\\
    &= \left \{ y \in \RR^M : \bigwedge_{i=1}^{D'_k} (b_{k_i}^T y = 0)
    \right \} .
    \end{aligned}

Note that :math:`y \in Z_{\UUU}` if and only if 
:math:`(y \in \UUU_1) \vee \dots \vee (y \in \UUU_K)`.
Alternatively:


.. math::
    \bigvee_{k=1}^K (y\in \UUU_k) \Leftrightarrow
    \bigvee_{k=1}^K \bigwedge_{j=1}^{D'_k} (b_{k_j}^T y = 0)
    \Leftrightarrow \bigwedge_{\sigma} \bigvee_{k=1}^K (b_{k_{\sigma(k)}}^T y = 0)

where :math:`\sigma` denotes an arbitrary choice of one normal vector
:math:`b_{k_{\sigma(k)}}` from each basis :math:`B_k` and we are considering
all such choices.
If :math:`y\in Z_{\UUU}`, it belongs to some :math:`\UUU_k`, 
and :math:`(b_{k_i}^T y = 0)` for each :math:`b_{k_i}`
in :math:`B_k`. Hence, for each choice :math:`\sigma`,
:math:`b_{k_{\sigma(k)}}^T y = 0` and RHS is true. Conversely,
assume RHS is true. If :math:`y \notin Z_{\UUU}`, then from each
:math:`B_k`, we could pick one normal vector :math:`b` such that :math:`b^T y \neq 0`.
This choice would make RHS false, hence :math:`y \in Z_{\UUU}`.
The total number of choices :math:`\sigma` is: :math:`\prod_{k=1}^K D'_k`.
Interestingly: 


.. math::
    \bigvee_{k=1}^K (b_{k_{\sigma(k)}}^T y = 0) \Leftrightarrow
    \left ( \prod_{k=1}^K (b_{k_{\sigma(k)}}^T y) = 0\right ) 
    \iff 
    (p^K_{\sigma}(y) = 0)

where :math:`p^K_{\sigma}(y)` is a homogeneous polynomial 
of degree :math:`K` in :math:`M` variables. 

Therefore, A union of :math:`K` subspaces can be represented as the zero set 
of a set of homogeneous polynomials  of the form:


.. math::
    :label: eq:pky_prod_linear

    p^K(y) = \prod_{k=1}^K (b_k^T y ) = c_K^T v_K(y),

where :math:`b_k \in \RR^M` is a normal vector to the :math:`k`-th
subspace and :math:`v_K(y)` is the Veronese embedding (see :ref:`here <sec:polynomial_rings>`)
of :math:`y \in \RR^M`
into :math:`\RR^{A_{K}(M)}`.
The problem of fitting :math:`K` subspaces to the given dataset is then
equivalent the problem of fitting homogeneous polynomials :math:`p^K(y)`
such that all the points in the dataset belong to the zero set of
these polynomials. Fitting of such polynomials doesn't require 
iterative data segmentation and model estimation since they depend
on all the points in the dataset. Once, the polynomials have been
identified, the remaining task is to split their zero-set into individual 
subspaces identified by :math:`B_k`.

In the following, we assume that the number of subspaces :math:`K` is known
beforehand. We consider the task of estimating :math:`K` later.
 
Fitting polynomials to data
""""""""""""""""""""""""""""""""""""""""""""""""""""""

Let :math:`I(Z_{\UUU})` be the vanishing ideal of :math:`Z_{\UUU}`. Since, the
number of subspaces :math:`K` is known, we only need to consider the
homogeneous component :math:`I_K` of :math:`I(Z_{\UUU})` 
:eq:`eq:graded_ring_subspace_arrangement`. 

The vanishing ideal :math:`I(\UUU_K)` of :math:`\UUU_k` is generated by 
the set of linear forms


.. math::
    \GGG_k = \{l(y) = b^T y, b \in B_k \}.
 
If the subspace arrangement is 
transversal, :math:`I_K` is generated by products of :math:`K` linear forms that
vanish on the :math:`K` subspaces.
Any polynomial :math:`p(y) \in I_K` can be written as a summation of
products of linear forms


.. math::
    p(y) = \sum l_1 (y) l_2(y) \dots l_K(y)

where :math:`l_k(y)` is a linear form in :math:`I(\UUU_k)`.
Using the Veronese map, each polynomial in :math:`I_K` can also be
written as:


.. math::
    p(y) = c_K^T v_K(y) = \sum c_{k_1, \dots, k_M} y_1^{k_1} \dots y_M^{k_M} = 0

where :math:`k_1 + \dots + k_M = K` and :math:`c_{k_1, \dots, k_M} \in \RR`
represents the coefficient of monomial :math:`y^{\underline{K}} = y_1^{k_1} \dots y_M^{k_M}`. Fitting the polynomial :math:`p(y)` is equivalent
to identifying its coefficient vector :math:`c_K`.
Since :math:`p(y) = 0` is satisfied by each data point :math:`y_s \in Y`, we have
:math:`c_K^T v_K(y_s) = 0` for all :math:`s = 1, \dots, S`. We define


.. math::
    V_K(M) = \begin{bmatrix}
    v_K(y_1)^T\\
    \vdots\\
    v_K(y_S)^T
    \end{bmatrix} \in \RR^{S \times A_K(M) }

as *embedded data matrix*. Then, we have


.. math::
    V_K(M) c_K = 0 \in \RR^S.

The coefficient vector :math:`c_K` of every polynomial in :math:`I_K` is 
in the null space of :math:`V_K(M)`. To ensure that every polynomial
obtained from :math:`V_K(M)` is in :math:`I_K`, we require that


.. math::
    \text{dim} (\NullSpace (V_K(M))) = \text{dim} (I_K) = h_I(K)

where :math:`h_I` is the Hilbert function of :math:`I(Z_{\UUU})` 
:eq:`eq:hilbert_function`. Equivalently, the rank of 
:math:`V_K(M)` needs to satisfy:


.. math::
    \text{rank}(V_K(M)) = A_K(M) -  h_I(K).

This condition is typically satisfied with
:math:`S \geq (A_K(M) - 1)` points in general position.  
Assuming this, a basis for :math:`I_K` can be constructed
from the set of :math:`h_I(K)` singular vectors of :math:`V_K(M)`
associated with its :math:`h_I(K)` zero singular values.
In the presence of moderate noise, we can still estimate
the coefficients of the polynomials in the least squares 
sense from the singular vectors associated with the 
:math:`h_I(K)` smallest singular values.

 
Subspaces by polynomial differentiation
""""""""""""""""""""""""""""""""""""""""""""""""""""""

Now that we have obtained a basis for the polynomials in :math:`I_K`,
the next step is to calculate the basis vectors :math:`B_k` for
each :math:`\UUU_k^{\perp}`.
 